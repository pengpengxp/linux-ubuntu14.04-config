一、安装sun的jdk和hadoop，不要使用open-jdk
本人安装的jdk1.7.0.rpm包（默认安装路劲为/usr/java/jdk1.7.0）
解压hadoop-0.20.2到：/home/hadoop/
以上两个包下载地址：jdk：http://download.oracle.com/otn-pub/java/jdk/7/jdk-7-linux-i586.rpm
hadoop-0.20.2：http://download.csdn.net/tag/hadoop-0.20.2
二、配置文件和环境变量
配置/etc/profile文件，追加环境变量如下：
export JAVA_HOME=”/usr/java/jdk1.7.0″
export JRE_HOME=”/usr/java/jdk1.7.0/jre”
export HADOOP_HOME=/home/hadoop1/hadoop-0.20.2
export CLASSPATH=.:$JAVA_HOME/LIB:$JRE_HOME/lib:$HADOOP_HOME/lib:$CLASSPATH
export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH
export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH
配置hadoop中/conf/hadoop-env.sh文件，追加环境变量如下：
export JAVA_HOME=”/usr/java/jdk1.7.0″
配置Hadoop安装目录/conf/core-site.xml文件
在configuration节点中增加以下内容：
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/tmp/hadoop/hadoop-${user.name}</value>
</property>
配置Hadoop安装目录/conf/hdfs-site.xml文件
在configuration节点中增加以下内容：
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
配置Hadoop安装目录/conf/mapred-site.xml文件
在configuration节点中增加以下内容：
<property>
<name>mapred.job.tracker</name>
<value>localhost:9001</value>
</property>
配置Hadoop安装目录/conf/mapred-site.xml文件
在configuration节点中增加以下内容：
<property>
<name>mapred.job.tracker</name>
<value>localhost:9001</value>
</property>
 
三、配置 SSH 免密码登陆(要记得是在hadoop用户下生成公钥和私钥，不是在root用户下)
1、ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/hadoop/.ssh/id_rsa): 默认路径
Enter passphrase (empty for no passphrase): 回车，空密码
Enter same passphrase again:回车，空密码
Your identification has been saved in /home/hadoop/.ssh/id_rsa.
Your public key has been saved in /home/hadoop/.ssh/id_rsa.pub.
2、cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
3、chmod 600 ~/.ssh/authorized_keys
4、sudo service sshd restart
如果此时出现如下问题：
We trust you have received the usual lecture from the local System
Administrator. It usually boils down to these three things:
#1) Respect the privacy of others.
#2) Think before you type.
#3) With great power comes great responsibility.
[sudo] password for hadoop1:
hadoop is not in the sudoers file.  This incident will be reported.
那按照如下操作把sudo权限赋予hadoop用户
[hadoop@localhost root]$ su -
Password:
[root@localhost ~]# chmod u+w /etc/sudoers
[root@localhost ~]# visudo
然后在里面这个地方加入hadoop1  ALL=(ALL)       ALL：
## Allow root to run any commands anywhere
root    ALL=(ALL)     ALL
hadoop  ALL=(ALL)       ALL
[root@localhost ~]# chmod u-w /etc/sudoers
然后su回hadoop用户
使用source使配置文件生效（/etc/profile、Hadoop安装目录/conf/hadoop-env.sh）
source /etc/profile
source /home/hadoop1/hadoop-0.20.2/conf/hadoop-env.sh
格式化nameNode(配置好环境就可以直接使用以下命令格式化nameNode)
hadoop namenode -format
运行Hadoop(配置好环境就可以直接使用以下命令启动脚本)
启动：start-all.sh
停止：stop-all.sh
创建目录：hadoop dfs -mkdir 目录名称
浏览目录：hadoop dfs -ls 目录名称
查看进程信息：jps
查看集群信息：hadoop dfsadmin -report
复制文件到目录：hadoop dfs -copyFromLocal 文件名称 目录名称
hadoop dfs 查询帮助
使用网页工具，浏览器输入：localhost:50030或localhost:50070查看相关信息

